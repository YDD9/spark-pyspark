{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Tuning\n",
    "\n",
    "https://spark.apache.org/docs/2.2.0/ml-tuning.html\n",
    "    ML tuning:\n",
    "        find good model\n",
    "        find good parameters of model\n",
    "\n",
    "https://mapr.com/blog/predicting-loan-credit-risk-using-apache-spark-machine-learning-random-forests/\n",
    "    another tuning example\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(id=4, text='spark i j k', probability=DenseVector([0.1136, 0.8864]), prediction=1.0)\n",
      "Row(id=5, text='l m n', probability=DenseVector([0.9964, 0.0036]), prediction=0.0)\n",
      "Row(id=6, text='mapreduce spark', probability=DenseVector([0.3073, 0.6927]), prediction=1.0)\n",
      "Row(id=7, text='apache hadoop', probability=DenseVector([0.9587, 0.0413]), prediction=0.0)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "# Prepare training documents, which are labeled.\n",
    "training = spark.createDataFrame([\n",
    "    (0, \"a b c d e spark\", 1.0),\n",
    "    (1, \"b d\", 0.0),\n",
    "    (2, \"spark f g h\", 1.0),\n",
    "    (3, \"hadoop mapreduce\", 0.0),\n",
    "    (4, \"b spark who\", 1.0),\n",
    "    (5, \"g d a y\", 0.0),\n",
    "    (6, \"spark fly\", 1.0),\n",
    "    (7, \"was mapreduce\", 0.0),\n",
    "    (8, \"e spark program\", 1.0),\n",
    "    (9, \"a e c l\", 0.0),\n",
    "    (10, \"spark compile\", 1.0),\n",
    "    (11, \"hadoop software\", 0.0)\n",
    "], [\"id\", \"text\", \"label\"])\n",
    "\n",
    "# Configure an ML pipeline, which consists of tree stages: tokenizer, hashingTF, and lr.\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")  # [\"was\", \"mapreduce\"]\n",
    "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n",
    "lr = LogisticRegression(maxIter=10)\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])\n",
    "\n",
    "# We now treat the Pipeline as an Estimator, wrapping it in a CrossValidator instance.\n",
    "# This will allow us to jointly choose parameters for all Pipeline stages.\n",
    "# A CrossValidator requires an Estimator, a set of Estimator ParamMaps, and an Evaluator.\n",
    "# We use a ParamGridBuilder to construct a grid of parameters to search over.\n",
    "# With 3 values for hashingTF.numFeatures and 2 values for lr.regParam,\n",
    "# this grid will have 3 x 2 = 6 parameter settings for CrossValidator to choose from.\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(hashingTF.numFeatures, [10, 100, 1000]) \\\n",
    "    .addGrid(lr.regParam, [0.1, 0.01]) \\\n",
    "    .build()\n",
    "\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=BinaryClassificationEvaluator(),\n",
    "                          numFolds=2)  # use 3+ folds in practice\n",
    "\n",
    "# Run cross-validation, and choose the best set of parameters.\n",
    "cvModel = crossval.fit(training)\n",
    "\n",
    "# Prepare test documents, which are unlabeled.\n",
    "test = spark.createDataFrame([\n",
    "    (4, \"spark i j k\"),\n",
    "    (5, \"l m n\"),\n",
    "    (6, \"mapreduce spark\"),\n",
    "    (7, \"apache hadoop\")\n",
    "], [\"id\", \"text\"])\n",
    "\n",
    "# Make predictions on test documents. cvModel uses the best model found (lrModel).\n",
    "prediction = cvModel.transform(test)\n",
    "selected = prediction.select(\"id\", \"text\", \"probability\", \"prediction\")\n",
    "for row in selected.collect():\n",
    "    print(row)\n",
    "    \n",
    "# spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tokenizer_43bc8deb11b72f601119"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+\n",
      "| id|            sentence|               words|\n",
      "+---+--------------------+--------------------+\n",
      "|  0|Hi I heard about ...|[hi, i, heard, ab...|\n",
      "|  1|I wish Java could...|[i, wish, java, c...|\n",
      "|  2|Logistic,regressi...|[logistic,regress...|\n",
      "+---+--------------------+--------------------+\n",
      "\n",
      "+-----------------------------------+------------------------------------------+------+\n",
      "|sentence                           |words                                     |tokens|\n",
      "+-----------------------------------+------------------------------------------+------+\n",
      "|Hi I heard about Spark             |[hi, i, heard, about, spark]              |5     |\n",
      "|I wish Java could use case classes |[i, wish, java, could, use, case, classes]|7     |\n",
      "|Logistic,regression,models,are,neat|[logistic,regression,models,are,neat]     |1     |\n",
      "+-----------------------------------+------------------------------------------+------+\n",
      "\n",
      "+-----------------------------------+------------------------------------------+------+\n",
      "|sentence                           |words                                     |tokens|\n",
      "+-----------------------------------+------------------------------------------+------+\n",
      "|Hi I heard about Spark             |[hi, i, heard, about, spark]              |5     |\n",
      "|I wish Java could use case classes |[i, wish, java, could, use, case, classes]|7     |\n",
      "|Logistic,regression,models,are,neat|[logistic, regression, models, are, neat] |5     |\n",
      "+-----------------------------------+------------------------------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://spark.apache.org/docs/2.2.0/ml-features.html#tokenizer\n",
    "\"\"\"\n",
    "Tokenization is the process of taking text (such as a sentence) and\n",
    "breaking it into individual terms (usually words). \n",
    "A simple Tokenizer class provides this functionality. \n",
    "The example below shows how to split sentences into sequences of words.\n",
    "\n",
    "RegexTokenizer allows more advanced tokenization based on regular expression (regex) matching. \n",
    "By default, the parameter “pattern” (regex, default: \"\\\\s+\") is used as delimiters to split the input text. \n",
    "Alternatively, users can set parameter “gaps” to false indicating the regex “pattern” denotes “tokens” rather \n",
    "than splitting gaps, and find all matching occurrences as the tokenization result.\n",
    "\"\"\"\n",
    "\n",
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "sentenceDataFrame = spark.createDataFrame([\n",
    "    (0, \"Hi I heard about Spark\"),\n",
    "    (1, \"I wish Java could use case classes\"),\n",
    "    (2, \"Logistic,regression,models,are,neat\")], [\"id\", \"sentence\"])\n",
    "\n",
    "# split by ' ', but ',' is not touched. return 'Tokenizer' object\n",
    "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")  \n",
    "display(tokenizer)\n",
    "\n",
    "# split by pattern '\\\\W' word\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"sentence\", outputCol=\"words\", \n",
    "                                pattern=\"\\\\W\")  # alternatively, pattern=\"\\\\w+\", gaps(False)\n",
    "\n",
    "countTokens = udf(lambda words: len(words), IntegerType())\n",
    "\n",
    "# tokenizer object transform together with a DataFrame, return a inner join DataFrame\n",
    "tokenized = tokenizer.transform(sentenceDataFrame)  \n",
    "tokenized.show()\n",
    "\n",
    "tokenized.select(\"sentence\", \"words\")\\\n",
    "    .withColumn(\"tokens\", countTokens(col(\"words\"))).show(truncate=False)  # choose 2 cols and add new one with update\n",
    "\n",
    "regexTokenized = regexTokenizer.transform(sentenceDataFrame)\n",
    "regexTokenized.select(\"sentence\", \"words\") \\\n",
    "    .withColumn(\"tokens\", countTokens(col(\"words\"))).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|       words|\n",
      "+------------+\n",
      "|[a, b, c, d]|\n",
      "|   [a, b, c]|\n",
      "|   [b, c, d]|\n",
      "+------------+\n",
      "\n",
      "+------------+--------------------+\n",
      "|       words|         features123|\n",
      "+------------+--------------------+\n",
      "|[a, b, c, d]|(8,[1,2,6],[1.0,2...|\n",
      "|   [a, b, c]| (8,[1,2],[1.0,2.0])|\n",
      "|   [b, c, d]|(8,[1,2,6],[1.0,1...|\n",
      "+------------+--------------------+\n",
      "\n",
      "[Row(words=['a', 'b', 'c', 'd'], features123=SparseVector(8, {1: 1.0, 2: 2.0, 6: 1.0})), Row(words=['a', 'b', 'c'], features123=SparseVector(8, {1: 1.0, 2: 2.0})), Row(words=['b', 'c', 'd'], features123=SparseVector(8, {1: 1.0, 2: 1.0, 6: 1.0}))]\n",
      "Row(words=['a', 'b', 'c', 'd'], features123=SparseVector(8, {1: 1.0, 2: 2.0, 6: 1.0}))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SparseVector(8, {1: 1.0, 2: 2.0, 6: 1.0})"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://spark.apache.org/docs/2.2.0/api/python/pyspark.ml.html#pyspark.ml.feature.HashingTF\n",
    "\"\"\"\n",
    "Maps a sequence of terms to their term frequencies using the hashing trick.   such as string to a hashed number/vector\n",
    "\n",
    "Currently we use Austin Appleby’s MurmurHash 3 algorithm (MurmurHash3_x86_32) to calculate \n",
    "the hash code value for the term object. Since a simple modulo is used to transform the hash \n",
    "function to a column index, it is advisable to use a power of two as the numFeatures parameter;\n",
    "otherwise the features will not be mapped evenly to the columns.\n",
    "\"\"\"\n",
    "\n",
    "from pyspark.ml.feature import HashingTF\n",
    "\n",
    "# ([],) must have extra ',' to work\n",
    "df = spark.createDataFrame([([\"a\", \"b\", \"c\", \"d\"],), ([\"a\", \"b\", \"c\"],), ([\"b\", \"c\", \"d\"],)], [\"words\"])\n",
    "df.show()\n",
    "\n",
    "hashingTF = HashingTF(numFeatures=8, inputCol=\"words\", outputCol=\"features123\")\n",
    "hashingTF.transform(df).show()\n",
    "\n",
    "print(hashingTF.transform(df).take(3))\n",
    "print(hashingTF.transform(df).head())\n",
    "hashingTF.transform(df).head().features123\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.cs.umd.edu/Outreach/hsContest99/questions/node3.html\n",
    "\n",
    "# Sparse Vectors\n",
    "A vector is a one-dimensional array of elements. The natural C++ implementation of a vector is as a one-dimensional array. However, in many applications, the elements of a vector have mostly zero values. Such a vector is said to be sparse. It is inefficient to use a one-dimensional array to store a sparse vector. It is also inefficient to add elements whose values are zero in forming sums of sparse vectors. Consequently, we should choose a different representation.\n",
    "\n",
    "One possibility is to represent the elements of a sparse vector as a linked list of nodes, each of which contains an integer index, a numerical value, and a pointer to the next node. Generally, the entries of the list will correspond to the non-zero elements of the vector in order, with each entry containing the index and value for that entry. (This restriction may be violated if a zero value is explicitly assigned to an element).\n",
    "\n",
    "Your goal is to write a program to add pairs of sparse vectors, creating new sparse vectors. The results of addition should not include any elements whose values are zero. You should then print the resulting vectors with elements in ascending order of index (from smallest index to largest).\n",
    "\n",
    "Input Format\n",
    "The input will be several pairs of sparse vectors, with each vector on a separate line. Each sparse vector will consist of a number of index-value pairs, where the first number in each pair is an integer representing the index (location), and the second number is a floating-point number representing the actual value. You may assume all index locations are non-negative. Elements will be entered in ascending order of index. The list of vectors is terminated by a line containing only -1.\n",
    "\n",
    "Output Format\n",
    "The output will be sparse vectors representing the sum of each pair of input vectors, each on a separate line. Vector elements should appear as pairs of indices and values, separated by a comma and a blank and enclosed in square braces. Vectors should appear as lists of elements separated by commas. The vector elements must be printed in ascending order of index. Vectors with no elements should appear as the string \"empty vector\".\n",
    "\n",
    "Example\n",
    "Input:\n",
    "\n",
    "3 1.0 2500 6.3 5000 10.0 60000 5.7 \n",
    "1 7.5 3 5.7 2500 -6.3 \n",
    "\n",
    "10 0.0 \n",
    "15000 6.7 \n",
    "\n",
    "100 -1.0\n",
    "100 1.0\n",
    "\n",
    "-1\n",
    "\n",
    "Output:\n",
    "\n",
    "[1, 7.5], [3, 6.7], [5000, 10], [60000, 5.7]\n",
    "\n",
    "[15000, 6.7]\n",
    "\n",
    "empty vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25.0\n",
      "22.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# http://spark.apache.org/docs/2.2.0/api/python/pyspark.ml.html#pyspark.ml.linalg.SparseVector\n",
    "# https://en.wikipedia.org/wiki/Sparse_matrix\n",
    "\"\"\"\n",
    "A simple sparse vector class for passing data to MLlib. Users may alternatively pass SciPy’s {scipy.sparse} data types.\n",
    "\n",
    "In numerical analysis and computer science, a sparse matrix or sparse array is a matrix in which most of the elements\n",
    "are zero. By contrast, if most of the elements are nonzero, then the matrix is considered dense. The number of \n",
    "zero-valued elements divided by the total number of elements (e.g., m × n for an m × n matrix) is called the sparsity \n",
    "of the matrix (which is equal to 1 minus the density of the matrix).\n",
    "\"\"\"\n",
    "from pyspark.ml.linalg import SparseVector\n",
    "import array\n",
    "\n",
    "a = SparseVector(4, [1, 3], [3.0, 4.0])\n",
    "print(a.dot(a))  # 25.0 = 0*0 + 3*3 + 4*4\n",
    "\n",
    "# https://docs.python.org/3/library/array.html\n",
    "# https://stackoverflow.com/questions/176011/python-list-vs-array-when-to-use\n",
    "arr = array.array('d', [1., 2., 3., 4.])  # 'd' type double\n",
    "print(a.dot(arr))  # 22.0 = 0*0 + 3*2 + 4*4\n",
    "\n",
    "b = SparseVector(4, [2], [1.0])\n",
    "a.dot(b)  # 0.0 = 0*0 + 3*0 + 4*0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+\n",
      "| id|    name|age|\n",
      "+---+--------+---+\n",
      "|  1|John Doe| 21|\n",
      "+---+--------+---+\n",
      "\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      "\n",
      "root\n",
      " |-- slen(name): integer (nullable = true)\n",
      " |-- to_upper(name): string (nullable = true)\n",
      " |-- add_one(age): integer (nullable = true)\n",
      "\n",
      "+----------+--------------+------------+\n",
      "|slen(name)|to_upper(name)|add_one(age)|\n",
      "+----------+--------------+------------+\n",
      "|         8|      JOHN DOE|          22|\n",
      "+----------+--------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# http://spark.apache.org/docs/2.2.0/api/python/pyspark.sql.html#pyspark.sql.functions.udf\n",
    "# Creates a Column expression representing a user defined function (UDF).\n",
    "\n",
    "from pyspark.sql.types import IntegerType\n",
    "# two ways to define UDF\n",
    "# also two ways to be used later, with/without .alias()\n",
    "slen = udf(lambda s: len(s), IntegerType())\n",
    "# @udf(returnType=IntegerType())\n",
    "# def slen(s):\n",
    "#     return len(s)\n",
    "@udf\n",
    "def to_upper(s):\n",
    "    if s is not None:\n",
    "        return s.upper()\n",
    "\n",
    "@udf(returnType=IntegerType())\n",
    "def add_one(x):\n",
    "    if x is not None:\n",
    "        return x + 1\n",
    "\n",
    "df = spark.createDataFrame([(1, \"John Doe\", 21)], (\"id\", \"name\", \"age\"))\n",
    "df.show()\n",
    "df.printSchema()\n",
    "\n",
    "df_new = df.select(slen(\"name\").alias(\"slen(name)\"), to_upper(\"name\"), add_one(\"age\"))\n",
    "# df_new = df.select(slen(\"name\"), to_upper(\"name\"), add_one(\"age\"))\n",
    "df_new.printSchema()\n",
    "df_new.show()\n",
    "\n",
    "# header are changed, \n",
    "# then column values are computed and updated.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
