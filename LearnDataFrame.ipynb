{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext(appName='LearnDataFrame')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save cars.csv to /predix/ for later usage.\n",
    "\n",
    "```\n",
    "year,make,model,comment,blank\n",
    "2012,Tesla,S,No comment,\n",
    ",,,,\n",
    "1997,Ford,E350,Go get one now they are going fast,\n",
    "2015,Chevy,Volt,,\n",
    "2017,Audi,QQ,,\n",
    "2018,Citroen,,,\n",
    "2012,Tesla,S,No comment,\n",
    ",,,,\n",
    "1997,Ford,E350,Go get one now they are going fast,\n",
    "2015,Chevy,Volt,,\n",
    "2017,Audi,QQ,,\n",
    "2018,Citroen,,,\n",
    "2012,Tesla,S,No comment,\n",
    "1997,Ford,E350,Go get one now they are going fast,\n",
    "2017,Audi,QQ,,\n",
    "2018,Citroen,,,\n",
    "2012,Tesla,S,No comment,\n",
    "2015,Chevy,Volt,,\n",
    "2017,Audi,QQ,,\n",
    "2018,Citroen,,,\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', 'I', 'AM']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = ['Hello' , 'I' , 'AM', 'Ankit ', 'Gupta']\n",
    "rdd = sc.parallelize(data)\n",
    "rdd.take(3)  # just print 3, safe choice when you have infi lines\n",
    "# rdd.collect()  # print all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year: integer (nullable = true)\n",
      " |-- make: string (nullable = true)\n",
      " |-- model: string (nullable = true)\n",
      " |-- comment: string (nullable = true)\n",
      " |-- blank: string (nullable = true)\n",
      "\n",
      "20\n",
      "\n",
      "\n",
      "[Row(year=2012, make='Tesla', model='S', comment='No comment', blank=None), Row(year=None, make=None, model=None, comment=None, blank=None)]\n",
      "\n",
      "\n",
      "+----+-------+-----+--------------------+-----+\n",
      "|year|   make|model|             comment|blank|\n",
      "+----+-------+-----+--------------------+-----+\n",
      "|2012|  Tesla|    S|          No comment| null|\n",
      "|null|   null| null|                null| null|\n",
      "|1997|   Ford| E350|Go get one now th...| null|\n",
      "|2015|  Chevy| Volt|                null| null|\n",
      "|2017|   Audi|   QQ|                null| null|\n",
      "|2018|Citroen| null|                null| null|\n",
      "|2012|  Tesla|    S|          No comment| null|\n",
      "|null|   null| null|                null| null|\n",
      "|1997|   Ford| E350|Go get one now th...| null|\n",
      "|2015|  Chevy| Volt|                null| null|\n",
      "|2017|   Audi|   QQ|                null| null|\n",
      "|2018|Citroen| null|                null| null|\n",
      "|2012|  Tesla|    S|          No comment| null|\n",
      "|1997|   Ford| E350|Go get one now th...| null|\n",
      "|2017|   Audi|   QQ|                null| null|\n",
      "|2018|Citroen| null|                null| null|\n",
      "|2012|  Tesla|    S|          No comment| null|\n",
      "|2015|  Chevy| Volt|                null| null|\n",
      "|2017|   Audi|   QQ|                null| null|\n",
      "|2018|Citroen| null|                null| null|\n",
      "+----+-------+-----+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)\n",
    "# https://www.analyticsvidhya.com/blog/2016/09/comprehensive-introduction-to-apache-spark-rdds-dataframes-using-pyspark/\n",
    "# train = sqlContext.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').load('train.csv')\n",
    "\n",
    "# https://github.com/databricks/spark-csv#python-api\n",
    "\n",
    "cars = sqlContext.read.format('csv').options(header='true', inferschema='true').load('cars.csv') # load CSV\n",
    "\n",
    "cars.printSchema()  # list headers as tree\n",
    "\n",
    "print(cars.count())  # num of rows in CSV\n",
    "print('\\n')\n",
    "\n",
    "print(cars.take(2))  # print first 2 rows as list\n",
    "print('\\n')\n",
    "\n",
    "cars.show()  # by default show first 20 lines as table\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+-----+-------+-----+\n",
      "|year|make|model|comment|blank|\n",
      "+----+----+-----+-------+-----+\n",
      "+----+----+-----+-------+-----+\n",
      "\n",
      "+----+-------+-----+--------------------+-----+\n",
      "|year|   make|model|             comment|blank|\n",
      "+----+-------+-----+--------------------+-----+\n",
      "|2012|  Tesla|    S|          No comment| null|\n",
      "|1997|   Ford| E350|Go get one now th...| null|\n",
      "|2015|  Chevy| Volt|                null| null|\n",
      "|2017|   Audi|   QQ|                null| null|\n",
      "|2018|Citroen| null|                null| null|\n",
      "|2012|  Tesla|    S|          No comment| null|\n",
      "|1997|   Ford| E350|Go get one now th...| null|\n",
      "|2015|  Chevy| Volt|                null| null|\n",
      "|2017|   Audi|   QQ|                null| null|\n",
      "|2018|Citroen| null|                null| null|\n",
      "|2012|  Tesla|    S|          No comment| null|\n",
      "|1997|   Ford| E350|Go get one now th...| null|\n",
      "|2017|   Audi|   QQ|                null| null|\n",
      "|2018|Citroen| null|                null| null|\n",
      "|2012|  Tesla|    S|          No comment| null|\n",
      "|2015|  Chevy| Volt|                null| null|\n",
      "|2017|   Audi|   QQ|                null| null|\n",
      "|2018|Citroen| null|                null| null|\n",
      "+----+-------+-----+--------------------+-----+\n",
      "\n",
      "+----+-------+-----+--------------------+-----+\n",
      "|year|   make|model|             comment|blank|\n",
      "+----+-------+-----+--------------------+-----+\n",
      "|2012|  Tesla|    S|          No comment|    -|\n",
      "|   0|      -|    -|                   -|    -|\n",
      "|1997|   Ford| E350|Go get one now th...|    -|\n",
      "|2015|  Chevy| Volt|                   -|    -|\n",
      "|2017|   Audi|   QQ|                   -|    -|\n",
      "|2018|Citroen|    -|                   -|    -|\n",
      "|2012|  Tesla|    S|          No comment|    -|\n",
      "|   0|      -|    -|                   -|    -|\n",
      "|1997|   Ford| E350|Go get one now th...|    -|\n",
      "|2015|  Chevy| Volt|                   -|    -|\n",
      "|2017|   Audi|   QQ|                   -|    -|\n",
      "|2018|Citroen|    -|                   -|    -|\n",
      "|2012|  Tesla|    S|          No comment|    -|\n",
      "|1997|   Ford| E350|Go get one now th...|    -|\n",
      "|2017|   Audi|   QQ|                   -|    -|\n",
      "|2018|Citroen|    -|                   -|    -|\n",
      "|2012|  Tesla|    S|          No comment|    -|\n",
      "|2015|  Chevy| Volt|                   -|    -|\n",
      "|2017|   Audi|   QQ|                   -|    -|\n",
      "|2018|Citroen|    -|                   -|    -|\n",
      "+----+-------+-----+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# By default, drop() method will drop a row if it contains any null value.\n",
    "# We can also pass ‘all” to drop a row only if all its values are null.\n",
    "cars.na.drop('any').show()\n",
    "cars.na.drop(\"all\").show()\n",
    "\n",
    "# na.fill() will replace null with the same type int by 0 and str by '-'\n",
    "cars.na.fill(0).na.fill('-').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-----+-----+--------------------+-----+\n",
      "|summary|              year| make|model|             comment|blank|\n",
      "+-------+------------------+-----+-----+--------------------+-----+\n",
      "|  count|                18|   18|   14|                   7|    0|\n",
      "|   mean|2012.4444444444443| null| null|                null| null|\n",
      "| stddev|    7.445663737823| null| null|                null| null|\n",
      "|    min|              1997| Audi| E350|Go get one now th...| null|\n",
      "|    max|              2018|Tesla| Volt|          No comment| null|\n",
      "+-------+------------------+-----+-----+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cars.describe().show()  # show statistics of DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(year=2012, model='S'),\n",
       " Row(year=None, model=None),\n",
       " Row(year=1997, model='E350'),\n",
       " Row(year=2015, model='Volt'),\n",
       " Row(year=2017, model='QQ'),\n",
       " Row(year=2018, model=None),\n",
       " Row(year=2012, model='S'),\n",
       " Row(year=None, model=None),\n",
       " Row(year=1997, model='E350'),\n",
       " Row(year=2015, model='Volt'),\n",
       " Row(year=2017, model='QQ'),\n",
       " Row(year=2018, model=None),\n",
       " Row(year=2012, model='S'),\n",
       " Row(year=1997, model='E350'),\n",
       " Row(year=2017, model='QQ'),\n",
       " Row(year=2018, model=None),\n",
       " Row(year=2012, model='S'),\n",
       " Row(year=2015, model='Volt'),\n",
       " Row(year=2017, model='QQ'),\n",
       " Row(year=2018, model=None)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cars.select('year', 'model').collect()  # get column by select\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cars.select('year').distinct().count()  # column select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|year|\n",
      "+----+\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cars.select('year').subtract(cars.select('year')).show()  # column subtract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SparkSession - a new entry point\n",
    "https://docs.databricks.com/spark/latest/gentle-introduction/sparksession.html\n",
    "\n",
    "We have been getting a lot of questions about thre relationship between SparkContext, SQLContext, and HiveContext in Spark 1.x. It was really strange to have \"HiveContext\" as an entry point when people want to use the DataFrame API. In Spark 2.0, we are introducing SparkSession, a new entry point that subsumes SQLContext and HiveContext. For backward compatibiilty, the two are preserved. SparkSession has many features, and here we demonstrate some of the more important ones.\n",
    "\n",
    "To read the companion blog post, click here: https://databricks.com/blog/2016/05/11/spark-2-0-technical-preview-easier-faster-and-smarter.html\n",
    "\n",
    "\n",
    "\n",
    "# Creating a SparkSession\n",
    "\n",
    "A SparkSession can be created using a builder pattern. The builder will automatically reuse an existing SparkContext if one exists; and create a SparkContext if it does not exist. Configuration options set in the builder are automatically propagated over to Spark and Hadoop during I/O.\n",
    "\n",
    "<!-- -->\n",
    "// A SparkSession can be created using a builder pattern\n",
    "import org.apache.spark.sql.SparkSession\n",
    "val sparkSession = SparkSession.builder\n",
    "  .master(\"local\")\n",
    "  .appName(\"my-spark-app\")\n",
    "  .config(\"spark.some.config.option\", \"config-value\")\n",
    "  .getOrCreate()\n",
    "  \n",
    "https://databricks.com/blog/2016/08/15/how-to-use-sparksession-in-apache-spark-2-0.html\n",
    "\n",
    "`from pyspark.sql import SparkSession`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# stringIndexer\n",
    "\n",
    "make a string to a number then you can select it for ML calculation as a feature.\n",
    "https://spark.apache.org/docs/2.2.0/ml-features.html#stringindexer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "| id|category|\n",
      "+---+--------+\n",
      "|  0|       a|\n",
      "|  1|       b|\n",
      "|  2|       c|\n",
      "|  3|       a|\n",
      "|  4|       a|\n",
      "|  5|       c|\n",
      "+---+--------+\n",
      "\n",
      "+---+--------+-------------+\n",
      "| id|category|categoryIndex|\n",
      "+---+--------+-------------+\n",
      "|  0|       a|          0.0|\n",
      "|  1|       b|          2.0|\n",
      "|  2|       c|          1.0|\n",
      "|  3|       a|          0.0|\n",
      "|  4|       a|          0.0|\n",
      "|  5|       c|          1.0|\n",
      "+---+--------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder. \\\n",
    "        appName('LearnDataFrame') \\\n",
    "        .getOrCreate()\n",
    "        \n",
    "df = spark.createDataFrame(\n",
    "    [(0, \"a\"), (1, \"b\"), (2, \"c\"), (3, \"a\"), (4, \"a\"), (5, \"c\")],\n",
    "    [\"id\", \"category\"])\n",
    "df.show()\n",
    "\n",
    "indexer = StringIndexer(inputCol=\"category\", outputCol=\"categoryIndex\")\n",
    "indexed = indexer.fit(df).transform(df)\n",
    "indexed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+-----+--------------------+-----+-----------+\n",
      "|year|   make|model|             comment|blank|makeIndexed|\n",
      "+----+-------+-----+--------------------+-----+-----------+\n",
      "|2012|  Tesla|    S|          No comment| null|        1.0|\n",
      "|1997|   Ford| E350|Go get one now th...| null|        4.0|\n",
      "|2015|  Chevy| Volt|                null| null|        3.0|\n",
      "|2017|   Audi|   QQ|                null| null|        0.0|\n",
      "|2018|Citroen| null|                null| null|        2.0|\n",
      "|2012|  Tesla|    S|          No comment| null|        1.0|\n",
      "|1997|   Ford| E350|Go get one now th...| null|        4.0|\n",
      "|2015|  Chevy| Volt|                null| null|        3.0|\n",
      "|2017|   Audi|   QQ|                null| null|        0.0|\n",
      "|2018|Citroen| null|                null| null|        2.0|\n",
      "|2012|  Tesla|    S|          No comment| null|        1.0|\n",
      "|1997|   Ford| E350|Go get one now th...| null|        4.0|\n",
      "|2017|   Audi|   QQ|                null| null|        0.0|\n",
      "|2018|Citroen| null|                null| null|        2.0|\n",
      "|2012|  Tesla|    S|          No comment| null|        1.0|\n",
      "|2015|  Chevy| Volt|                null| null|        3.0|\n",
      "|2017|   Audi|   QQ|                null| null|        0.0|\n",
      "|2018|Citroen| null|                null| null|        2.0|\n",
      "+----+-------+-----+--------------------+-----+-----------+\n",
      "\n",
      "+-----+--------+\n",
      "|label|features|\n",
      "+-----+--------+\n",
      "| 2012|     1.0|\n",
      "| 1997|     4.0|\n",
      "| 2015|     3.0|\n",
      "| 2017|     0.0|\n",
      "| 2018|     2.0|\n",
      "| 2012|     1.0|\n",
      "| 1997|     4.0|\n",
      "| 2015|     3.0|\n",
      "| 2017|     0.0|\n",
      "| 2018|     2.0|\n",
      "| 2012|     1.0|\n",
      "| 1997|     4.0|\n",
      "| 2017|     0.0|\n",
      "| 2018|     2.0|\n",
      "| 2012|     1.0|\n",
      "| 2015|     3.0|\n",
      "| 2017|     0.0|\n",
      "| 2018|     2.0|\n",
      "+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://stackoverflow.com/questions/34681534/spark-ml-stringindexer-handling-unseen-labels\n",
    "\n",
    "makeIndexer = StringIndexer(inputCol=\"make\", outputCol=\"makeIndexed\", \\\n",
    "                            handleInvalid=\"skip\") # options are \"keep\", \"error\" or \"skip\" to handle null\n",
    "\n",
    "# otherwise below error returned.\n",
    "# Caused by: org.apache.spark.SparkException: StringIndexer encountered NULL value. To handle or skip NULLS, \n",
    "# try setting StringIndexer.handleInvalid.\n",
    "\n",
    "makeIndexed = makeIndexer.fit(cars).transform(cars)\n",
    "makeIndexed.show()\n",
    "\n",
    "makeIndexed.select('year', 'makeIndexed').toDF('label', 'features').show()  # change column name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
