{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext(appName='LearnDataFrame')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save cars.csv to /predix/ for later usage.\n",
    "\n",
    "```\n",
    "year,make,model,comment,blank\n",
    "2012,Tesla,S,No comment,\n",
    ",,,,\n",
    "1997,Ford,E350,Go get one now they are going fast,\n",
    "2015,Chevy,Volt,,\n",
    "2017,Audi,QQ,,\n",
    "2018,Citroen,,,\n",
    "2012,Tesla,S,No comment,\n",
    ",,,,\n",
    "1997,Ford,E350,Go get one now they are going fast,\n",
    "2015,Chevy,Volt,,\n",
    "2017,Audi,QQ,,\n",
    "2018,Citroen,,,\n",
    "2012,Tesla,S,No comment,\n",
    "1997,Ford,E350,Go get one now they are going fast,\n",
    "2017,Audi,QQ,,\n",
    "2018,Citroen,,,\n",
    "2012,Tesla,S,No comment,\n",
    "2015,Chevy,Volt,,\n",
    "2017,Audi,QQ,,\n",
    "2018,Citroen,,,\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', 'I', 'AM']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = ['Hello' , 'I' , 'AM', 'Ankit ', 'Gupta']\n",
    "rdd = sc.parallelize(data)\n",
    "rdd.take(3)  # just print 3, safe choice when you have infi lines\n",
    "# rdd.collect()  # print all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year: integer (nullable = true)\n",
      " |-- make: string (nullable = true)\n",
      " |-- model: string (nullable = true)\n",
      " |-- comment: string (nullable = true)\n",
      " |-- blank: string (nullable = true)\n",
      "\n",
      "20\n",
      "\n",
      "\n",
      "[Row(year=2012, make='Tesla', model='S', comment='No comment', blank=None), Row(year=None, make=None, model=None, comment=None, blank=None)]\n",
      "\n",
      "\n",
      "+----+-------+-----+--------------------+-----+\n",
      "|year|   make|model|             comment|blank|\n",
      "+----+-------+-----+--------------------+-----+\n",
      "|2012|  Tesla|    S|          No comment| null|\n",
      "|null|   null| null|                null| null|\n",
      "|1997|   Ford| E350|Go get one now th...| null|\n",
      "|2015|  Chevy| Volt|                null| null|\n",
      "|2017|   Audi|   QQ|                null| null|\n",
      "|2018|Citroen| null|                null| null|\n",
      "|2012|  Tesla|    S|          No comment| null|\n",
      "|null|   null| null|                null| null|\n",
      "|1997|   Ford| E350|Go get one now th...| null|\n",
      "|2015|  Chevy| Volt|                null| null|\n",
      "|2017|   Audi|   QQ|                null| null|\n",
      "|2018|Citroen| null|                null| null|\n",
      "|2012|  Tesla|    S|          No comment| null|\n",
      "|1997|   Ford| E350|Go get one now th...| null|\n",
      "|2017|   Audi|   QQ|                null| null|\n",
      "|2018|Citroen| null|                null| null|\n",
      "|2012|  Tesla|    S|          No comment| null|\n",
      "|2015|  Chevy| Volt|                null| null|\n",
      "|2017|   Audi|   QQ|                null| null|\n",
      "|2018|Citroen| null|                null| null|\n",
      "+----+-------+-----+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)\n",
    "# https://www.analyticsvidhya.com/blog/2016/09/comprehensive-introduction-to-apache-spark-rdds-dataframes-using-pyspark/\n",
    "# train = sqlContext.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').load('train.csv')\n",
    "\n",
    "# https://github.com/databricks/spark-csv#python-api\n",
    "\n",
    "cars = sqlContext.read.format('csv').options(header='true', inferschema='true').load('cars.csv') # load CSV\n",
    "\n",
    "cars.printSchema()  # list headers as tree\n",
    "\n",
    "print(cars.count())  # num of rows in CSV\n",
    "print('\\n')\n",
    "\n",
    "print(cars.take(2))  # print first 2 rows as list\n",
    "print('\\n')\n",
    "\n",
    "cars.show()  # by default show first 20 lines as table\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+-----+-------+-----+\n",
      "|year|make|model|comment|blank|\n",
      "+----+----+-----+-------+-----+\n",
      "+----+----+-----+-------+-----+\n",
      "\n",
      "+----+-------+-----+--------------------+-----+\n",
      "|year|   make|model|             comment|blank|\n",
      "+----+-------+-----+--------------------+-----+\n",
      "|2012|  Tesla|    S|          No comment| null|\n",
      "|1997|   Ford| E350|Go get one now th...| null|\n",
      "|2015|  Chevy| Volt|                null| null|\n",
      "|2017|   Audi|   QQ|                null| null|\n",
      "|2018|Citroen| null|                null| null|\n",
      "|2012|  Tesla|    S|          No comment| null|\n",
      "|1997|   Ford| E350|Go get one now th...| null|\n",
      "|2015|  Chevy| Volt|                null| null|\n",
      "|2017|   Audi|   QQ|                null| null|\n",
      "|2018|Citroen| null|                null| null|\n",
      "|2012|  Tesla|    S|          No comment| null|\n",
      "|1997|   Ford| E350|Go get one now th...| null|\n",
      "|2017|   Audi|   QQ|                null| null|\n",
      "|2018|Citroen| null|                null| null|\n",
      "|2012|  Tesla|    S|          No comment| null|\n",
      "|2015|  Chevy| Volt|                null| null|\n",
      "|2017|   Audi|   QQ|                null| null|\n",
      "|2018|Citroen| null|                null| null|\n",
      "+----+-------+-----+--------------------+-----+\n",
      "\n",
      "+----+-------+-----+--------------------+-----+\n",
      "|year|   make|model|             comment|blank|\n",
      "+----+-------+-----+--------------------+-----+\n",
      "|2012|  Tesla|    S|          No comment|    -|\n",
      "|   0|      -|    -|                   -|    -|\n",
      "|1997|   Ford| E350|Go get one now th...|    -|\n",
      "|2015|  Chevy| Volt|                   -|    -|\n",
      "|2017|   Audi|   QQ|                   -|    -|\n",
      "|2018|Citroen|    -|                   -|    -|\n",
      "|2012|  Tesla|    S|          No comment|    -|\n",
      "|   0|      -|    -|                   -|    -|\n",
      "|1997|   Ford| E350|Go get one now th...|    -|\n",
      "|2015|  Chevy| Volt|                   -|    -|\n",
      "|2017|   Audi|   QQ|                   -|    -|\n",
      "|2018|Citroen|    -|                   -|    -|\n",
      "|2012|  Tesla|    S|          No comment|    -|\n",
      "|1997|   Ford| E350|Go get one now th...|    -|\n",
      "|2017|   Audi|   QQ|                   -|    -|\n",
      "|2018|Citroen|    -|                   -|    -|\n",
      "|2012|  Tesla|    S|          No comment|    -|\n",
      "|2015|  Chevy| Volt|                   -|    -|\n",
      "|2017|   Audi|   QQ|                   -|    -|\n",
      "|2018|Citroen|    -|                   -|    -|\n",
      "+----+-------+-----+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# By default, drop() method will drop a row if it contains any null value.\n",
    "# We can also pass ‘all” to drop a row only if all its values are null.\n",
    "cars.na.drop('any').show()\n",
    "cars.na.drop(\"all\").show()\n",
    "\n",
    "# na.fill() will replace null with the same type int by 0 and str by '-'\n",
    "cars.na.fill(0).na.fill('-').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-----+-----+--------------------+-----+\n",
      "|summary|              year| make|model|             comment|blank|\n",
      "+-------+------------------+-----+-----+--------------------+-----+\n",
      "|  count|                18|   18|   14|                   7|    0|\n",
      "|   mean|2012.4444444444443| null| null|                null| null|\n",
      "| stddev|    7.445663737823| null| null|                null| null|\n",
      "|    min|              1997| Audi| E350|Go get one now th...| null|\n",
      "|    max|              2018|Tesla| Volt|          No comment| null|\n",
      "+-------+------------------+-----+-----+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cars.describe().show()  # show statistics of DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(year=2012, model='S'),\n",
       " Row(year=None, model=None),\n",
       " Row(year=1997, model='E350'),\n",
       " Row(year=2015, model='Volt'),\n",
       " Row(year=2017, model='QQ'),\n",
       " Row(year=2018, model=None),\n",
       " Row(year=2012, model='S'),\n",
       " Row(year=None, model=None),\n",
       " Row(year=1997, model='E350'),\n",
       " Row(year=2015, model='Volt'),\n",
       " Row(year=2017, model='QQ'),\n",
       " Row(year=2018, model=None),\n",
       " Row(year=2012, model='S'),\n",
       " Row(year=1997, model='E350'),\n",
       " Row(year=2017, model='QQ'),\n",
       " Row(year=2018, model=None),\n",
       " Row(year=2012, model='S'),\n",
       " Row(year=2015, model='Volt'),\n",
       " Row(year=2017, model='QQ'),\n",
       " Row(year=2018, model=None)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cars.select('year', 'model').collect()  # get column by select\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cars.select('year').distinct().count()  # column select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|year|\n",
      "+----+\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cars.select('year').subtract(cars.select('year')).show()  # column subtract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SparkSession - a new entry point\n",
    "https://docs.databricks.com/spark/latest/gentle-introduction/sparksession.html\n",
    "\n",
    "We have been getting a lot of questions about thre relationship between SparkContext, SQLContext, and HiveContext in Spark 1.x. It was really strange to have \"HiveContext\" as an entry point when people want to use the DataFrame API. In Spark 2.0, we are introducing SparkSession, a new entry point that subsumes SQLContext and HiveContext. For backward compatibiilty, the two are preserved. SparkSession has many features, and here we demonstrate some of the more important ones.\n",
    "\n",
    "To read the companion blog post, click here: https://databricks.com/blog/2016/05/11/spark-2-0-technical-preview-easier-faster-and-smarter.html\n",
    "\n",
    "\n",
    "\n",
    "# Creating a SparkSession\n",
    "\n",
    "A SparkSession can be created using a builder pattern. The builder will automatically reuse an existing SparkContext if one exists; and create a SparkContext if it does not exist. Configuration options set in the builder are automatically propagated over to Spark and Hadoop during I/O.\n",
    "\n",
    "<!-- -->\n",
    "// A SparkSession can be created using a builder pattern\n",
    "import org.apache.spark.sql.SparkSession\n",
    "val sparkSession = SparkSession.builder\n",
    "  .master(\"local\")\n",
    "  .appName(\"my-spark-app\")\n",
    "  .config(\"spark.some.config.option\", \"config-value\")\n",
    "  .getOrCreate()\n",
    "  \n",
    "https://databricks.com/blog/2016/08/15/how-to-use-sparksession-in-apache-spark-2-0.html\n",
    "\n",
    "`from pyspark.sql import SparkSession`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# stringIndexer\n",
    "\n",
    "make a string to a number then you can select it for ML calculation as a feature.\n",
    "https://spark.apache.org/docs/2.2.0/ml-features.html#stringindexer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "| id|category|\n",
      "+---+--------+\n",
      "|  0|       a|\n",
      "|  1|       b|\n",
      "|  2|       c|\n",
      "|  3|       a|\n",
      "|  4|       a|\n",
      "|  5|       c|\n",
      "+---+--------+\n",
      "\n",
      "+---+--------+-------------+\n",
      "| id|category|categoryIndex|\n",
      "+---+--------+-------------+\n",
      "|  0|       a|          0.0|\n",
      "|  1|       b|          2.0|\n",
      "|  2|       c|          1.0|\n",
      "|  3|       a|          0.0|\n",
      "|  4|       a|          0.0|\n",
      "|  5|       c|          1.0|\n",
      "+---+--------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder. \\\n",
    "        appName('LearnDataFrame') \\\n",
    "        .getOrCreate()\n",
    "        \n",
    "df = spark.createDataFrame(\n",
    "    [(0, \"a\"), (1, \"b\"), (2, \"c\"), (3, \"a\"), (4, \"a\"), (5, \"c\")],\n",
    "    [\"id\", \"category\"])\n",
    "df.show()\n",
    "\n",
    "indexer = StringIndexer(inputCol=\"category\", outputCol=\"categoryIndex\")\n",
    "indexed = indexer.fit(df).transform(df)\n",
    "indexed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+-----+--------------------+-----+-----------+\n",
      "|year|   make|model|             comment|blank|makeIndexed|\n",
      "+----+-------+-----+--------------------+-----+-----------+\n",
      "|2012|  Tesla|    S|          No comment| null|        1.0|\n",
      "|1997|   Ford| E350|Go get one now th...| null|        4.0|\n",
      "|2015|  Chevy| Volt|                null| null|        3.0|\n",
      "|2017|   Audi|   QQ|                null| null|        0.0|\n",
      "|2018|Citroen| null|                null| null|        2.0|\n",
      "|2012|  Tesla|    S|          No comment| null|        1.0|\n",
      "|1997|   Ford| E350|Go get one now th...| null|        4.0|\n",
      "|2015|  Chevy| Volt|                null| null|        3.0|\n",
      "|2017|   Audi|   QQ|                null| null|        0.0|\n",
      "|2018|Citroen| null|                null| null|        2.0|\n",
      "|2012|  Tesla|    S|          No comment| null|        1.0|\n",
      "|1997|   Ford| E350|Go get one now th...| null|        4.0|\n",
      "|2017|   Audi|   QQ|                null| null|        0.0|\n",
      "|2018|Citroen| null|                null| null|        2.0|\n",
      "|2012|  Tesla|    S|          No comment| null|        1.0|\n",
      "|2015|  Chevy| Volt|                null| null|        3.0|\n",
      "|2017|   Audi|   QQ|                null| null|        0.0|\n",
      "|2018|Citroen| null|                null| null|        2.0|\n",
      "+----+-------+-----+--------------------+-----+-----------+\n",
      "\n",
      "+-----+--------+\n",
      "|label|features|\n",
      "+-----+--------+\n",
      "| 2012|     1.0|\n",
      "| 1997|     4.0|\n",
      "| 2015|     3.0|\n",
      "| 2017|     0.0|\n",
      "| 2018|     2.0|\n",
      "| 2012|     1.0|\n",
      "| 1997|     4.0|\n",
      "| 2015|     3.0|\n",
      "| 2017|     0.0|\n",
      "| 2018|     2.0|\n",
      "| 2012|     1.0|\n",
      "| 1997|     4.0|\n",
      "| 2017|     0.0|\n",
      "| 2018|     2.0|\n",
      "| 2012|     1.0|\n",
      "| 2015|     3.0|\n",
      "| 2017|     0.0|\n",
      "| 2018|     2.0|\n",
      "+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://stackoverflow.com/questions/34681534/spark-ml-stringindexer-handling-unseen-labels\n",
    "\n",
    "makeIndexer = StringIndexer(inputCol=\"make\", outputCol=\"makeIndexed\", \\\n",
    "                            handleInvalid=\"skip\") # options are \"keep\", \"error\" or \"skip\" to handle null\n",
    "\n",
    "# otherwise below error returned.\n",
    "# Caused by: org.apache.spark.SparkException: StringIndexer encountered NULL value. To handle or skip NULLS, \n",
    "# try setting StringIndexer.handleInvalid.\n",
    "\n",
    "makeIndexed = makeIndexer.fit(cars).transform(cars)\n",
    "makeIndexed.show()\n",
    "\n",
    "makeIndexed.select('year', 'makeIndexed').toDF('label', 'features').show()  # change column name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+\n",
      "|label|features|\n",
      "+-----+--------+\n",
      "| 2012|     1.0|\n",
      "| 1997|     4.0|\n",
      "| 2015|     3.0|\n",
      "| 2017|     0.0|\n",
      "| 2018|     2.0|\n",
      "| 2012|     1.0|\n",
      "| 1997|     4.0|\n",
      "| 2015|     3.0|\n",
      "| 2017|     0.0|\n",
      "| 2018|     2.0|\n",
      "| 2012|     1.0|\n",
      "| 1997|     4.0|\n",
      "| 2017|     0.0|\n",
      "| 2018|     2.0|\n",
      "| 2012|     1.0|\n",
      "| 2015|     3.0|\n",
      "| 2017|     0.0|\n",
      "| 2018|     2.0|\n",
      "+-----+--------+\n",
      "\n"
     ]
    },
    {
     "ename": "IllegalArgumentException",
     "evalue": "'requirement failed: Column features must be of type org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7 but was actually DoubleType.'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o539.fit.\n: java.lang.IllegalArgumentException: requirement failed: Column features must be of type org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7 but was actually DoubleType.\n\tat scala.Predef$.require(Predef.scala:224)\n\tat org.apache.spark.ml.util.SchemaUtils$.checkColumnType(SchemaUtils.scala:42)\n\tat org.apache.spark.ml.PredictorParams$class.validateAndTransformSchema(Predictor.scala:51)\n\tat org.apache.spark.ml.Predictor.validateAndTransformSchema(Predictor.scala:82)\n\tat org.apache.spark.ml.Predictor.transformSchema(Predictor.scala:144)\n\tat org.apache.spark.ml.PipelineStage.transformSchema(Pipeline.scala:74)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:100)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-f417219b7067>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain_cv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_cv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselectedFeature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandomSplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mmodel1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_cv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m     62\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \"\"\"\n\u001b[1;32m    261\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mQueryExecutionException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'java.lang.IllegalArgumentException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mIllegalArgumentException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: 'requirement failed: Column features must be of type org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7 but was actually DoubleType.'"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "rf = RandomForestRegressor()\n",
    "\n",
    "selectedFeature = makeIndexed.select('year', 'makeIndexed').toDF('label', 'features')\n",
    "selectedFeature.show()\n",
    "(train_cv, test_cv) = selectedFeature.randomSplit([0.7, 0.3])\n",
    "\n",
    "model1 = rf.fit(train_cv)\n",
    "\n",
    "# https://mapr.com/blog/predicting-loan-credit-risk-using-apache-spark-machine-learning-random-forests/\n",
    "# https://archive.ics.uci.edu/ml/datasets/Statlog+(German+Credit+Data)     data download link\n",
    "\n",
    "# https://spark.apache.org/docs/2.2.0/mllib-ensembles.html#classification\n",
    "# https://stackoverflow.com/questions/30298523/spark-create-rdd-of-label-features-pairs-from-csv-file     data download link\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
