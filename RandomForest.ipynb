{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build random forest\n",
    "spark RF mllib\n",
    "https://spark.apache.org/docs/2.2.0/mllib-ensembles.html#random-forests\n",
    "https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/    data download link \n",
    "https://stackoverflow.com/questions/30298523/spark-create-rdd-of-label-features-pairs-from-csv-file    how to prepare data\n",
    "\n",
    "tune RF for better performance guide\n",
    "https://www.analyticsvidhya.com/blog/2015/06/tuning-random-forest-model/\n",
    "\n",
    "what is random forest ?\n",
    "https://www.coursera.org/learn/python-machine-learning/lecture/lF9QN/random-forests\n",
    "Typically, for a classification problem with p features, âˆšp (rounded down) features are used in each split.[3]:592 For regression problems the inventors recommend p/3 (rounded down) with a minimum node size of 5 as the default. wiki\n",
    "training set bagging + feature bagging\n",
    "\n",
    "implement random forest from scratch\n",
    "https://www.youtube.com/watch?v=QHOazyP-YlM\n",
    "\n",
    "\n",
    "off topic\n",
    "Algorithms Specialization\n",
    "Learn To Think Like A Computer Scientist. Master the fundamentals of the design and analysis of algorithms.\n",
    "https://www.coursera.org/specializations/algorithms?action=enroll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example German credit\n",
    "https://mapr.com/blog/predicting-loan-credit-risk-using-apache-spark-machine-learning-random-forests/\n",
    "https://archive.ics.uci.edu/ml/datasets/Statlog+(German+Credit+Data)     data download link\n",
    "\n",
    "A very good comparison using various model to determin diabetes, also explain the features in the model\n",
    "https://towardsdatascience.com/machine-learning-for-diabetes-562dd7df4d42\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncompressing a .Z file\n",
    "Files ending in .Z have been compressed using the compress command. Compressed files can be restored to their original form using the uncompress command. Type:\n",
    "```\n",
    "        uncompress filename.tar.Z \n",
    "```\n",
    "This replaces filename.tar.Z with filename.tar\n",
    "\n",
    "Unzipping a .gz file\n",
    "Files ending in .gz have been compressed using the gzip command. Zipped files can be restored to their original form using the gunzip command. Type:\n",
    "```\n",
    "         gunzip filename.tar.gz \n",
    "```\n",
    "This replaces filename.tar.gz with filename.tar\n",
    "\n",
    "Extracting tar files\n",
    "The tar command is used for packing several files or even a whole directory into a single tarfile.\n",
    "To unpack a tar file, type:\n",
    "```\n",
    "         tar xvf filename.tar \n",
    "```\n",
    "Note: Always move to an empty directory before unpacking a tarfile. \n",
    "When you unpack a tarfile its contents are written to the current directory. Any file or directory with the same name as the contents of the tarfile will be overwritten!\n",
    "\n",
    "A quick method\n",
    "Alternatively, you can unpack a compressed/zipped tarfile using the zcat or gzcat command. (gzcat is a local alias for the GNU zcat command). This leaves the compressed .Z or .gz tarfile intact. For example:\n",
    "```\n",
    "        zcat filename.tar.Z  | tar xvf - \n",
    "# or\n",
    "       gzcat filename.tar.gz | tar xvf -\n",
    "```\n",
    "The zcat command recreates the uncompressed tarfile, which is then piped into the tar command to extract the files.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* diabetes-data.tar.Z contains the distribution for 70 sets of data recorded\n",
    "on diabetes patients (several weeks' to months' worth of glucose, insulin,\n",
    "and lifestyle data per patient + a description of the problem domain).\n",
    "Archived using tar and compressed.\n",
    "\n",
    "The Code field is deciphered as follows:\n",
    "\n",
    "33 = Regular insulin dose\n",
    "34 = NPH insulin dose\n",
    "35 = UltraLente insulin dose\n",
    "\n",
    "48 = Unspecified blood glucose measurement\n",
    "57 = Unspecified blood glucose measurement\n",
    "58 = Pre-breakfast blood glucose measurement\n",
    "59 = Post-breakfast blood glucose measurement\n",
    "60 = Pre-lunch blood glucose measurement\n",
    "61 = Post-lunch blood glucose measurement\n",
    "62 = Pre-supper blood glucose measurement\n",
    "63 = Post-supper blood glucose measurement\n",
    "64 = Pre-snack blood glucose measurement\n",
    "\n",
    "65 = Hypoglycemic symptoms\n",
    "\n",
    "66 = Typical meal ingestion\n",
    "67 = More-than-usual meal ingestion\n",
    "68 = Less-than-usual meal ingestion\n",
    "\n",
    "69 = Typical exercise activity\n",
    "70 = More-than-usual exercise activity\n",
    "71 = Less-than-usual exercise activity\n",
    "\n",
    "72 = Unspecified special event\n",
    "\n",
    "\n",
    "https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary/diabetes\n",
    "the data is already cleaned for machine learning, but do not have feature names\n",
    "```\n",
    "-1  1:6.000000 2:148.000000 3:72.000000 4:35.000000 5:0.000000 6:33.599998 7:0.627000 8:50.000000\n",
    "```\n",
    "\n",
    "https://github.com/susanli2016/Machine-Learning-with-Python/blob/master/diabetes.csv\n",
    "a user has identify how the correspondance of data\n",
    "```\n",
    "Pregnancies\tGlucose\tBloodPressure\tSkinThickness\tInsulin\tBMI\tDiabetesPedigreeFunction\tAge\tOutcome\n",
    "6\t148\t72\t35\t0\t33.6\t0.627\t50\t1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.mllib.tree import RandomForest, RandomForestModel\n",
    "from pyspark.mllib.util import MLUtils\n",
    "\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext.getOrCreate()\n",
    "\n",
    "# model output dir setup\n",
    "import os\n",
    "import shutil\n",
    "outputDir = '/predix/target/tmp/myRandomForestClassificationModel/'\n",
    "if os.path.exists(outputDir):\n",
    "    shutil.rmtree(outputDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csvRDD:769\n",
      "['Pregnancies,Glucose,BloodPressure,SkinThickness,Insulin,BMI,DiabetesPedigreeFunction,Age,Outcome', '6,148,72,35,0,33.6,0.627,50,1', '1,85,66,29,0,26.6,0.351,31,0', '8,183,64,0,0,23.3,0.672,32,1', '1,89,66,23,94,28.1,0.167,21,0', '0,137,40,35,168,43.1,2.288,33,1']\n",
      "\n",
      "Pregnancies,Glucose,BloodPressure,SkinThickness,Insulin,BMI,DiabetesPedigreeFunction,Age,Outcome\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Option A: load a csv with headers and convert to LIBSVM format for ML RF\n",
    "#\n",
    "    \n",
    "# Load and parse the data file into an RDD\n",
    "csvRDD = sc.textFile(\"/media/sf_Windows_Share/diabetes.csv\")\n",
    "print('csvRDD:{}\\n{}\\n'.format(csvRDD.count(), csvRDD.take(6)))\n",
    "\n",
    "# To find the headers\n",
    "# first() method returns a string not a Rdd. \n",
    "header = csvRDD.first()  # or csvRDD.take(1)\n",
    "print(header)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['6,148,72,35,0,33.6,0.627,50,1', '1,85,66,29,0,26.6,0.351,31,0']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# Option A: option 1.1)\n",
    "#\n",
    "\n",
    "# remove hearder\n",
    "csvRDD = csvRDD.filter(lambda line: line != header)\n",
    "csvRDD.take(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sv.parallelize([header]):1\n",
      "['Pregnancies,Glucose,BloodPressure,SkinThickness,Insulin,BMI,DiabetesPedigreeFunction,Age,Outcome']\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['0,137,40,35,168,43.1,2.288,33,1',\n",
       " '10,168,74,0,0,38,0.537,34,1',\n",
       " '5,166,72,19,175,25.8,0.587,51,1',\n",
       " '0,118,84,47,230,45.8,0.551,31,1',\n",
       " '1,115,70,30,96,34.6,0.529,32,1',\n",
       " '7,147,76,0,0,39.4,0.257,43,1']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# Option A: option 1.2)\n",
    "#\n",
    "\n",
    "# convert header to rdd by using parallelize.\n",
    "header = sc.parallelize([header])\n",
    "print('sv.parallelize([header]):{}\\n{}\\n'.format(header.count(), header.take(6)))\n",
    "# To remove the header subtract Rdd only, Rdd seems has no order\n",
    "csvRDD = csvRDD.subtract(header)\n",
    "csvRDD.take(6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LabeledPoint(1.0, [6.0,148.0,72.0,35.0,0.0,33.6,0.627,50.0]), LabeledPoint(0.0, [1.0,85.0,66.0,29.0,0.0,26.6,0.351,31.0])]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Option A: load a csv with headers and convert to LIBSVM format for ML RF\n",
    "#\n",
    "\n",
    "# http://www.techpoweredmath.com/spark-dataframes-mllib-tutorial/#.WlFRNlunE2w\n",
    "\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "def createLabeledPoint(line):\n",
    "    features = line.split(',')\n",
    "    return LabeledPoint(features[8], features[0:8])  # yield can't be used here, only return work\n",
    "# https://spark.apache.org/docs/2.1.0/mllib-data-types.html#labeled-point\n",
    "\n",
    "    \n",
    "data = csvRDD.map(createLabeledPoint)\n",
    "print(data.take(2))\n",
    "\n",
    "\n",
    "# //To create a RDD of (label, features) pairs\n",
    "# val parsedData = data.map { line =>\n",
    "#     val parts = line.split(',')\n",
    "#     LabeledPoint(parts(0).toDouble, Vectors.dense(parts(1).split(' ').map(_.toDouble)))\n",
    "#     }.cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LabeledPoint(0.0, (8,[0,1,2,3,4,5,6,7],[6.0,148.0,72.0,35.0,0.0,33.599998,0.627,50.0])), LabeledPoint(1.0, (8,[0,1,2,3,4,5,6,7],[1.0,85.0,66.0,29.0,0.0,26.6,0.351,31.0]))]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Option B: Directly use data with LIBSVM format. \n",
    "#\n",
    "\n",
    "# Load and parse the data file into an RDD of LabeledPoint.\n",
    "# https://spark.apache.org/docs/2.2.0/mllib-data-types.html#labeled-point\n",
    "data = MLUtils.loadLibSVMFile(sc, '/media/sf_Windows_Share/diabetesLIBSVM.csv')\n",
    "\n",
    "# downloaeded LIBSVMFILE scale is [-1,+1] which won't work, change [-1, 0]\n",
    "# LibSVMFile example: \n",
    "# 0   1:6.000000 2:148.000000 3:72.000000 4:35.000000 5:0.000000 6:33.599998 7:0.627000 8:50.000000\n",
    "# +1  1:1.000000 2:85.000000 3:66.000000 4:29.000000 5:0.000000 6:26.600000 7:0.351000 8:31.000000\n",
    "# 0   1:8.000000 2:183.000000 3:64.000000 4:0.000000 5:0.000000 6:23.299999 7:0.672000 8:32.000000\n",
    "# +1  1:1.000000 2:89.000000 3:66.000000 4:23.000000 5:94.000000 6:28.100000 7:0.167000 8:21.000000\n",
    "                                \n",
    "print(data.take(2))\n",
    "\n",
    "# loading data into a structure: first column is label, rest are indexed features:\n",
    "# [LabeledPoint(0.0, (8,[0,1,2,3,4,5,6,7],[6.0,148.0,72.0,35.0,0.0,33.599998,0.627,50.0])),\n",
    "#  LabeledPoint(1.0, (8,[0,1,2,3,4,5,6,7],[1.0,85.0,66.0,29.0,0.0,26.6,0.351,31.0]))]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0,0.0,3.0]\n",
      "1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0, 1000), (1, 1001), (2, 1002), (3, 1003), (4, 1004)]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Knowledge: LabeledPoint, features, label\n",
    "x = LabeledPoint(1.0, [1.0, 0.0, 3.0])\n",
    "print(x.features)\n",
    "print(x.label)\n",
    "\n",
    "# Knowledge: rdd1.zip(rdd2)\n",
    "# https://spark.apache.org/docs/1.1.1/api/python/pyspark.rdd-pysrc.html#RDD.zip\n",
    "# https://www.safaribooksonline.com/library/view/learning-spark/9781449359034/ch04.html\n",
    "x = sc.parallelize(range(0,5))\n",
    "y = sc.parallelize(range(1000, 1005))\n",
    "x.zip(y).collect()  # [(0, 1000), (1, 1001), (2, 1002), (3, 1003), (4, 1004)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error = 0.26540284360189575\n",
      "Learned classification forest model:\n",
      "TreeEnsembleModel classifier with 3 trees\n",
      "\n",
      "  Tree 0:\n",
      "    If (feature 5 <= 29.8)\n",
      "     If (feature 7 <= 31.0)\n",
      "      If (feature 1 <= 105.0)\n",
      "       Predict: 0.0\n",
      "      Else (feature 1 > 105.0)\n",
      "       If (feature 0 <= 7.0)\n",
      "        Predict: 0.0\n",
      "       Else (feature 0 > 7.0)\n",
      "        Predict: 1.0\n",
      "     Else (feature 7 > 31.0)\n",
      "      If (feature 1 <= 155.0)\n",
      "       If (feature 6 <= 0.839)\n",
      "        Predict: 0.0\n",
      "       Else (feature 6 > 0.839)\n",
      "        Predict: 1.0\n",
      "      Else (feature 1 > 155.0)\n",
      "       If (feature 7 <= 62.0)\n",
      "        Predict: 1.0\n",
      "       Else (feature 7 > 62.0)\n",
      "        Predict: 0.0\n",
      "    Else (feature 5 > 29.8)\n",
      "     If (feature 7 <= 30.0)\n",
      "      If (feature 5 <= 42.4)\n",
      "       If (feature 4 <= 392.0)\n",
      "        Predict: 0.0\n",
      "       Else (feature 4 > 392.0)\n",
      "        Predict: 1.0\n",
      "      Else (feature 5 > 42.4)\n",
      "       If (feature 1 <= 94.0)\n",
      "        Predict: 0.0\n",
      "       Else (feature 1 > 94.0)\n",
      "        Predict: 1.0\n",
      "     Else (feature 7 > 30.0)\n",
      "      If (feature 6 <= 0.126)\n",
      "       If (feature 2 <= 70.0)\n",
      "        Predict: 0.0\n",
      "       Else (feature 2 > 70.0)\n",
      "        Predict: 0.0\n",
      "      Else (feature 6 > 0.126)\n",
      "       If (feature 5 <= 40.5)\n",
      "        Predict: 1.0\n",
      "       Else (feature 5 > 40.5)\n",
      "        Predict: 1.0\n",
      "  Tree 1:\n",
      "    If (feature 7 <= 28.0)\n",
      "     If (feature 1 <= 127.0)\n",
      "      If (feature 2 <= 92.0)\n",
      "       If (feature 3 <= 29.0)\n",
      "        Predict: 0.0\n",
      "       Else (feature 3 > 29.0)\n",
      "        Predict: 0.0\n",
      "      Else (feature 2 > 92.0)\n",
      "       Predict: 1.0\n",
      "     Else (feature 1 > 127.0)\n",
      "      If (feature 0 <= 3.0)\n",
      "       If (feature 3 <= 21.0)\n",
      "        Predict: 1.0\n",
      "       Else (feature 3 > 21.0)\n",
      "        Predict: 1.0\n",
      "      Else (feature 0 > 3.0)\n",
      "       If (feature 4 <= 0.0)\n",
      "        Predict: 1.0\n",
      "       Else (feature 4 > 0.0)\n",
      "        Predict: 0.0\n",
      "    Else (feature 7 > 28.0)\n",
      "     If (feature 1 <= 140.0)\n",
      "      If (feature 0 <= 12.0)\n",
      "       If (feature 1 <= 103.0)\n",
      "        Predict: 0.0\n",
      "       Else (feature 1 > 103.0)\n",
      "        Predict: 0.0\n",
      "      Else (feature 0 > 12.0)\n",
      "       If (feature 7 <= 46.0)\n",
      "        Predict: 1.0\n",
      "       Else (feature 7 > 46.0)\n",
      "        Predict: 0.0\n",
      "     Else (feature 1 > 140.0)\n",
      "      If (feature 5 <= 31.3)\n",
      "       If (feature 1 <= 155.0)\n",
      "        Predict: 1.0\n",
      "       Else (feature 1 > 155.0)\n",
      "        Predict: 1.0\n",
      "      Else (feature 5 > 31.3)\n",
      "       If (feature 0 <= 6.0)\n",
      "        Predict: 1.0\n",
      "       Else (feature 0 > 6.0)\n",
      "        Predict: 1.0\n",
      "  Tree 2:\n",
      "    If (feature 0 <= 6.0)\n",
      "     If (feature 1 <= 114.0)\n",
      "      If (feature 6 <= 0.702)\n",
      "       If (feature 2 <= 62.0)\n",
      "        Predict: 0.0\n",
      "       Else (feature 2 > 62.0)\n",
      "        Predict: 0.0\n",
      "      Else (feature 6 > 0.702)\n",
      "       If (feature 5 <= 37.0)\n",
      "        Predict: 0.0\n",
      "       Else (feature 5 > 37.0)\n",
      "        Predict: 0.0\n",
      "     Else (feature 1 > 114.0)\n",
      "      If (feature 5 <= 27.8)\n",
      "       If (feature 6 <= 0.141)\n",
      "        Predict: 0.0\n",
      "       Else (feature 6 > 0.141)\n",
      "        Predict: 0.0\n",
      "      Else (feature 5 > 27.8)\n",
      "       If (feature 7 <= 26.0)\n",
      "        Predict: 0.0\n",
      "       Else (feature 7 > 26.0)\n",
      "        Predict: 1.0\n",
      "    Else (feature 0 > 6.0)\n",
      "     If (feature 3 <= 19.0)\n",
      "      If (feature 5 <= 24.8)\n",
      "       If (feature 1 <= 119.0)\n",
      "        Predict: 0.0\n",
      "       Else (feature 1 > 119.0)\n",
      "        Predict: 1.0\n",
      "      Else (feature 5 > 24.8)\n",
      "       If (feature 6 <= 0.587)\n",
      "        Predict: 0.0\n",
      "       Else (feature 6 > 0.587)\n",
      "        Predict: 1.0\n",
      "     Else (feature 3 > 19.0)\n",
      "      If (feature 1 <= 144.0)\n",
      "       If (feature 3 <= 27.0)\n",
      "        Predict: 1.0\n",
      "       Else (feature 3 > 27.0)\n",
      "        Predict: 0.0\n",
      "      Else (feature 1 > 144.0)\n",
      "       Predict: 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# run RF model\n",
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = data.randomSplit([0.7, 0.3])\n",
    "\n",
    "# Train a RandomForest model.\n",
    "# Empty categoricalFeaturesInfo indicates all features are continuous.\n",
    "# In pyspark you would need a syntax like this n:m , where n is the column of categorical Features(such as \"pass\", \"BMW\",\n",
    "# which algo can't use, needs converting to a number, in some algo it must a continuous number)\n",
    "# m is the number of categories minus 1, and you can have multiple columns with \n",
    "# categorical variables each seperated with a comma.\n",
    "# categoricalFeaturesInfo={0:12,2:10,.....,10:2}\n",
    "\n",
    "#  Note: Use larger numTrees in practice.\n",
    "#  Setting featureSubsetStrategy=\"auto\" lets the algorithm choose.\n",
    "model = RandomForest.trainClassifier(trainingData, numClasses=2, categoricalFeaturesInfo={},\n",
    "                                     numTrees=3, featureSubsetStrategy=\"auto\",\n",
    "                                     impurity='gini', maxDepth=4, maxBins=32, seed=1)\n",
    "\n",
    "# Evaluate model on test instances and compute test error\n",
    "predictions = model.predict(testData.map(lambda x: x.features))\n",
    "labelsAndPredictions = testData.map(lambda lp: lp.label).zip(predictions)\n",
    "testErr = labelsAndPredictions.filter(lambda lp: lp[0] != lp[1]).count() / float(testData.count())\n",
    "print('Test Error = ' + str(testErr))\n",
    "print('Learned classification forest model:')\n",
    "print(model.toDebugString())\n",
    "\n",
    "# Save and load model\n",
    "model.save(sc, outputDir)\n",
    "sameModel = RandomForestModel.load(sc, outputDir)\n",
    "\n",
    "# sc.stop()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "testData:\n",
      "[LabeledPoint(1.0, [6.0,148.0,72.0,35.0,0.0,33.6,0.627,50.0]), LabeledPoint(1.0, [10.0,168.0,74.0,0.0,0.0,38.0,0.537,34.0]), LabeledPoint(1.0, [7.0,100.0,0.0,0.0,0.0,30.0,0.484,32.0]), LabeledPoint(1.0, [1.0,115.0,70.0,30.0,96.0,34.6,0.529,32.0]), LabeledPoint(1.0, [9.0,119.0,80.0,35.0,0.0,29.0,0.263,29.0]), LabeledPoint(1.0, [10.0,125.0,70.0,26.0,115.0,31.1,0.205,41.0]), LabeledPoint(0.0, [1.0,97.0,66.0,15.0,140.0,23.2,0.487,22.0]), LabeledPoint(0.0, [5.0,117.0,92.0,0.0,0.0,34.1,0.337,38.0]), LabeledPoint(0.0, [3.0,88.0,58.0,11.0,54.0,24.8,0.267,22.0])]\n",
      "\n",
      "predictions for testData:\n",
      "[1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0]\n",
      "\n",
      "labelsAndPredictions:\n",
      "[(1.0, 1.0), (1.0, 1.0), (1.0, 0.0), (1.0, 1.0), (1.0, 0.0), (1.0, 1.0), (0.0, 0.0), (0.0, 1.0), (0.0, 0.0)]\n",
      "\n",
      "labelsAndPredictions - get only wrong prediction:\n",
      "[(1.0, 0.0), (1.0, 0.0), (0.0, 1.0), (0.0, 1.0), (0.0, 1.0), (1.0, 0.0), (0.0, 1.0), (1.0, 0.0), (1.0, 0.0)]\n",
      "\n",
      "use saved model to predict testData:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 9\n",
    "\n",
    "print('\\ntestData:')\n",
    "print(testData.take(n))\n",
    "\n",
    "print('\\npredictions for testData:')\n",
    "print(predictions.take(n))\n",
    "\n",
    "print('\\nlabelsAndPredictions:')\n",
    "print(labelsAndPredictions.take(n))\n",
    "\n",
    "print('\\nlabelsAndPredictions - get only wrong prediction:')\n",
    "print(labelsAndPredictions.filter(lambda lp: lp[0] != lp[1]).take(n))\n",
    "\n",
    "print('\\nuse saved model to predict testData:') \n",
    "sameModel.predict(testData.map(lambda x: x.features)).take(n)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under PR = 0.6588483503114162\n",
      "Area under ROC = 0.6814048760991207\n"
     ]
    }
   ],
   "source": [
    "# https://spark.apache.org/docs/2.2.0/mllib-evaluation-metrics.html#binary-classification\n",
    "\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "\n",
    "# Compute raw scores on the test set\n",
    "predictionAndLabels = predictions.zip(testData.map(lambda lp: lp.label))\n",
    "\n",
    "# Instantiate metrics object\n",
    "metrics = BinaryClassificationMetrics(predictionAndLabels)\n",
    "\n",
    "# Area under precision-recall curve\n",
    "print(\"Area under PR = %s\" % metrics.areaUnderPR)\n",
    "\n",
    "# Area under ROC curve\n",
    "print(\"Area under ROC = %s\" % metrics.areaUnderROC)\n",
    "\n",
    "\n",
    "# http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.98.4362&rep=rep1&type=pdf\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html\n",
    "http://www.chioka.in/differences-between-roc-auc-and-pr-auc/\n",
    "\n",
    "**Precision** is defined as the number of true positives over the number of true positives plus the number of false positives.\n",
    "In simple words, the Precesion is a ratio presents: \n",
    "    how many positive prediction are true positive / how many positive predictions given\n",
    "    (the correctness of the positive prediction)\n",
    "\n",
    "**Recall** is defined as the number of true positives over the number of true positives plus the number of false negatives.\n",
    "In simple words, the Recall is a ration presents:\n",
    "    how many positive prediction are true positive / how many positive really exist\n",
    "    (the ability of discoverying positive)\n",
    "    \n",
    "the harmonic mean of precision and recall ==> PR\n",
    "\n",
    "Note that the precision may not decrease with recall. The definition of precision shows that lowering the threshold of a classifier may increase the denominator, by increasing the number of results returned. If the threshold was previously set too high, the new results may all be true positives, which will increase precision. If the previous threshold was about right or too low, further lowering the threshold will introduce false positives, decreasing precision.\n",
    "\n",
    "Recall is defined as , where TP+FN does not depend on the classifier threshold. This means that lowering the classifier threshold may increase recall, by increasing the number of true positive results. It is also possible that lowering the threshold may leave recall unchanged, while the precision fluctuates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "scikit trun RF blackbox more clearly:\n",
    "http://blog.datadive.net/interpreting-random-forests/\n",
    "http://blog.datadive.net/random-forest-interpretation-with-scikit-learn/\n",
    "\n",
    "House Data\n",
    "https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.names\n",
    "Attribute Information:\n",
    "\n",
    "    1. CRIM      per capita crime rate by town\n",
    "    2. ZN        proportion of residential land zoned for lots over \n",
    "                 25,000 sq.ft.\n",
    "    3. INDUS     proportion of non-retail business acres per town\n",
    "    4. CHAS      Charles River dummy variable (= 1 if tract bounds \n",
    "                 river; 0 otherwise)\n",
    "    5. NOX       nitric oxides concentration (parts per 10 million)\n",
    "    6. RM        average number of rooms per dwelling\n",
    "    7. AGE       proportion of owner-occupied units built prior to 1940\n",
    "    8. DIS       weighted distances to five Boston employment centres\n",
    "    9. RAD       index of accessibility to radial highways\n",
    "    10. TAX      full-value property-tax rate per 10,000\n",
    "    11. PTRATIO  pupil-teacher ratio by town\n",
    "    12. B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks \n",
    "                 by town\n",
    "    13. LSTAT    % lower status of the population\n",
    "    14. MEDV     Median value of owner-occupied homes in $1000's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "https://mapr.com/blog/predicting-loan-credit-risk-using-apache-spark-machine-learning-random-forests/\n",
    "\n",
    "\n",
    "Using an ML pipeline\n",
    "We will next train the model using a pipeline, which can give better results. A pipeline provides a simple way to try out different combinations of parameters, using a process called grid search, where you set up the parameters to test, and MLLib will test all the combinations. Pipelines make it easy to tune an entire model building workflow at once, rather than tuning each element in the Pipeline separately.\n",
    "\n",
    "\n",
    "https://spark.apache.org/docs/2.2.0/ml-tuning.html\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
