
# installation and start guide
https://www.datacamp.com/community/tutorials/apache-spark-tutorial-machine-learning


Python 3.6.3 | Anaconda
Spark 2.2.0


```python
import os
print(os.getcwd())
```

    /predix



```python
try:
    del sc  # delete the existing SparkContext, can't run multiple ones.
except:
    pass

import findspark
findspark.init()

import pyspark
sc = pyspark.SparkContext(appName='SC')

distData = sc.parallelize(range(10**9))
print(distData.take(10))

sc.stop()
```

    [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]



```python
# official example to calculate Pi approximately
# https://spark.apache.org/examples.html
del sc

import findspark
findspark.init('/usr/local/spark')
# https://github.com/minrk/findspark

import pyspark
import random

sc = pyspark.SparkContext(appName='myPi')
num_samples= 10 ** 9

def insdie(p):
    x, y = random.random(), random.random()
    return x*x + y*y < 1

count = sc.parallelize(range(0, num_samples)).filter(insdie).count()

pi = 4 * count / num_samples
print(pi)

sc.stop()
```

    3.141519496



```python
# official example text search https://spark.apache.org/examples.html
# https://spark.apache.org/docs/2.2.0/rdd-programming-guide.html

try:
    del sc 
except:
    pass

import findspark
findspark.init()

import pyspark

sc = pyspark.SparkContext(appName='wordsCount')
# sc = pyspark.SparkContext.getOrCreate()
lines = sc.textFile('log.log')

linesLength = lines.map(lambda s: len(s))
# persist() would cause lineLengths to be saved in memory after the first time it is computed.
# You can mark an RDD to be persisted using the persist() or cache() methods on it. 
totalLength = linesLength.persist().reduce(lambda a, b: a + b)
print('Process starts')
print(totalLength)

print(lines.first())
print(lines.take(5))

def wordsCount(s):
    return len(s.split(" "))
words = lines.map(wordsCount).reduce(lambda a,b: a + b)
print(words)


sc.stop()
print('Process ends')

```

    Process starts
    1567089
    [INFO] Scanning for projects...
    ['[INFO] Scanning for projects...', '[INFO] ', '[INFO] ------------------------------------------------------------------------', '[INFO] Building genericframework-service 2.0.15', '[INFO] ------------------------------------------------------------------------']
    90743
    Process ends



```python
# https://spark.apache.org/docs/2.1.1/streaming-programming-guide.html

# https://community.hortonworks.com/articles/81359/pyspark-streaming-wordcount-example.html

# spark streaming :  Stream processing doesn't have to imply, or require, "fast data" or "big data". 
# It can just mean processing data continually as it arrives, and not artificially splitting it into batches.

# https://www.rittmanmead.com/blog/2017/01/getting-started-with-spark-streaming-with-python-and-kafka/
import findspark
findspark.init()

from pyspark import SparkContext
from pyspark.streaming import StreamingContext

sc = SparkContext(appName='streamingWordsCount')
ssc = StreamingContext(sc, 60)   # batch interval in seconds 60
lines = ssc.textFileStream("log.log")
lines.pprint()
# https://github.com/vaquarkhan/vk-wiki-notes/wiki/Difference-between-flatMap()-and-map()-on-an-RDD
words = lines.flatMap(lambda line: line.split(" "))
pairs = words.map(lambda word: (word, 1))
wordCounts = pairs.reduceByKey(lambda x, y: x + y)

# def process(time, rdd):
#     df = sqlContext.createDataFrame(rdd)
#     df.registerTempTable("myCounts")

# wordCounts.foreachRDD(process)
wordCounts.pprint()

ssc.start()             # Start the computation
ssc.awaitTermination()  # Wait for the computation to terminate

sc.stop()
```

    -------------------------------------------
    Time: 2018-03-04 23:17:00
    -------------------------------------------
    
    -------------------------------------------
    Time: 2018-03-04 23:17:00
    -------------------------------------------
    
    -------------------------------------------
    Time: 2018-03-04 23:18:00
    -------------------------------------------
    
    -------------------------------------------
    Time: 2018-03-04 23:18:00
    -------------------------------------------
    
    -------------------------------------------
    Time: 2018-03-04 23:19:00
    -------------------------------------------
    
    -------------------------------------------
    Time: 2018-03-04 23:19:00
    -------------------------------------------
    
    -------------------------------------------
    Time: 2018-03-04 23:20:00
    -------------------------------------------
    
    -------------------------------------------
    Time: 2018-03-04 23:20:00
    -------------------------------------------
    



```python
try:
    del sc
    try:
        del scc
    except:
        scc.stop()
        pass
except:
    pass
```
